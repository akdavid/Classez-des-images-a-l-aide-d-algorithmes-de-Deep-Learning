{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <!-- TITLE --> Projet 6 : notebook 4\n",
    "<!-- AUTHOR : Anthony DAVID -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys, pathlib, json, glob\n",
    "import math, random\n",
    "import datetime\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from skimage.morphology import disk\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters import rank\n",
    "from skimage import io, color, exposure, transform\n",
    "from IPython.display import display,Image,Markdown,HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['Rottweiler', 'Doberman', 'Weimaraner', 'Staff', 'Malinois']\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Dossier de destination contenant les images copiées\n",
    "destination_dir = './selected_data/images/'\n",
    "\n",
    "# Vérifier si le dossier de destination existe et récupérer les noms des classes\n",
    "if os.path.exists(destination_dir):\n",
    "    class_names = [breed_dir for breed_dir in os.listdir(destination_dir) if os.path.isdir(os.path.join(destination_dir, breed_dir))]\n",
    "else:\n",
    "    class_names = []\n",
    "\n",
    "# Nombre de classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Affichage\n",
    "print(\"Class names:\", class_names)\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Full convolutions\n",
    "\n",
    "Here we will use many models with many datasets and then show a report in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_dir = f'./data'\n",
    "run_dir = './run_full'\n",
    "\n",
    "# ---- \n",
    "datasets      = ['set-BW', 'set-L', 'set-L-HE', 'set-L-LHE', 'set-L-CLAHE', 'set-RGB', 'set-RGB-HE']\n",
    "# models        = {'v1':'get_model_v1', 'v2':'get_model_v2', 'v3':'get_model_v3', 'v4':'get_model_v4', 'v5':'get_model_v5'}\n",
    "models        = {'v1':'get_model_v1'}\n",
    "batch_size    = 16\n",
    "epochs        = 20\n",
    "with_datagen  = False\n",
    "fit_verbosity = 0\n",
    "tag_id = '{:06}'.format(random.randint(0,99999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_np_dataset(*data):\n",
    "    \"\"\"\n",
    "    Shuffle a list of dataset\n",
    "    args:\n",
    "        *data : datasets\n",
    "    return:\n",
    "        *datasets mixed\n",
    "    \"\"\"\n",
    "    # print('Datasets have been shuffled.')\n",
    "    p = np.random.permutation(len(data[0]))\n",
    "    out = [ d[p] for d in data ]\n",
    "    return out[0] if len(out)==1 else out\n",
    "\n",
    "\n",
    "def read_dataset(enhanced_dir, dataset_name):\n",
    "    \"\"\"Reads h5 dataset from dataset_dir\"\"\"\n",
    "    filename = f'{enhanced_dir}/{dataset_name}.h5'\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        x_train = f['x_train'][:]\n",
    "        y_train = f['y_train'][:]\n",
    "        x_val = f['x_val'][:]\n",
    "        y_val = f['y_val'][:]\n",
    "        x_test = f['x_test'][:]\n",
    "        y_test = f['y_test'][:]\n",
    "    x_train, y_train = shuffle_np_dataset(x_train, y_train)\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Models collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_v1(lx,ly,lz):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), padding='same', input_shape=(lx,ly,lz), activation='relu'))\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
    "    model.add( keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten())\n",
    "    model.add( keras.layers.Dense(512, activation='relu'))\n",
    "    model.add( keras.layers.Dropout(0.4))\n",
    "    model.add( keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_v2(lx,ly,lz):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,lz)))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Flatten()) \n",
    "    model.add( keras.layers.Dense(1152, activation='relu'))\n",
    "    model.add( keras.layers.Dropout(0.4))\n",
    "\n",
    "    model.add( keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_v3(lx,ly,lz):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add( keras.layers.Conv2D(32, (3,3),   activation='relu', input_shape=(lx,ly,lz)))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add( keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    \n",
    "    model.add( keras.layers.GlobalAveragePooling2D())\n",
    "    model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add( keras.layers.Dense(1152, activation='relu'))\n",
    "    model.add( keras.layers.Dropout(0.4))\n",
    "\n",
    "    model.add( keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_v4(lx,ly,lz):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv2D(128, 4, activation='relu', input_shape=(lx,ly,lz)))\n",
    "    model.add(keras.layers.MaxPooling2D())\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(64, 4, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D())\n",
    "\n",
    "    model.add(keras.layers.Conv2D(32, 4, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D())\n",
    "\n",
    "    model.add(keras.layers.Conv2D(16, 4, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D())\n",
    "\n",
    "    model.add(keras.layers.Flatten()) \n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_v5(lx,ly,lz):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, (5, 5), padding='same',  activation='relu', input_shape=(lx,ly,lz)))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))      \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same',  activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 - Prepare the multi-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "055225\n"
     ]
    }
   ],
   "source": [
    "print(tag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_file = f'{run_dir}/checkpoint_{tag_id}.json'  # To store progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load checkpoint or initialize new run\n",
    "# def load_checkpoint():\n",
    "#     if os.path.exists(resume_file):\n",
    "#         with open(resume_file, 'r') as f:\n",
    "#             return json.load(f)\n",
    "#     else:\n",
    "#         return {'Dataset': [], 'Model': {}}\n",
    "\n",
    "# checkpoint = load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_run(enhanced_dir, datasets, models, datagen=None, \n",
    "              batch_size=batch_size, epochs=epochs, \n",
    "              fit_verbosity=0, tag_id='last'):\n",
    "    \"\"\"\n",
    "    Launches a dataset-model combination with checkpointing to resume in case of crashes.\n",
    "    args:\n",
    "        enhanced_dir   : Directory of the enhanced datasets\n",
    "        datasets       : List of dataset (whitout .h5)\n",
    "        models         : List of model like { \"model name\":get_model(), ...}\n",
    "        datagen        : Data generator or None (None)\n",
    "        batch_size     : Batch size (64)\n",
    "        epochs         : Number of epochs (16)\n",
    "        fit_verbosity  : Verbose level (0)\n",
    "        tag_id         : postfix for report, logs and models dir (_last)\n",
    "    return:\n",
    "        report        : Report as a dict for Pandas.\n",
    "    \"\"\"  \n",
    "    checkpoint_file = f'{run_dir}/checkpoint_{tag_id}.json'\n",
    "    \n",
    "    # Load existing checkpoint if available\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint = json.load(f)\n",
    "    else:\n",
    "        checkpoint = {\"Dataset\": [], \"Model\": {}}\n",
    "    \n",
    "    # ---- Logs and models dir\n",
    "    os.makedirs(f'{run_dir}/logs_{tag_id}', mode=0o750, exist_ok=True)\n",
    "    os.makedirs(f'{run_dir}/models_{tag_id}', mode=0o750, exist_ok=True)\n",
    "    \n",
    "    # ---- Columns of output\n",
    "    output = {}\n",
    "    output['Dataset'] = []\n",
    "    for m in models:\n",
    "        output[m+'_Accuracy'] = []\n",
    "        output[m+'_Duration'] = []\n",
    "\n",
    "    # ---- Let's go\n",
    "    for d_name in datasets:\n",
    "        # Skip dataset if already completed\n",
    "        if d_name in checkpoint[\"Dataset\"]:\n",
    "            print(f\"Skipping dataset {d_name} (already completed)\")\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nDataset : \",d_name)\n",
    "\n",
    "        # ---- Read dataset\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = read_dataset(enhanced_dir, d_name)\n",
    "        output['Dataset'].append(d_name)\n",
    "                \n",
    "        # ---- Get the shape\n",
    "        (n,lx,ly,lz) = x_train.shape\n",
    "\n",
    "        # ---- For each model\n",
    "        for m_name, m_function in models.items():\n",
    "            if d_name in checkpoint[\"Model\"] and m_name in checkpoint[\"Model\"][d_name]:\n",
    "                print(f\"    Skipping model {m_name} for dataset {d_name} (already completed)\")\n",
    "                continue   \n",
    "                     \n",
    "            print(\"    Run model {}  : \".format(m_name), end='')\n",
    "            # ---- get model\n",
    "            try:\n",
    "                # ---- get function by name\n",
    "                m_function=globals()[m_function]\n",
    "                model=m_function(lx,ly,lz)\n",
    "                # ---- Compile it\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), \n",
    "                              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                # ---- Callbacks tensorboard\n",
    "                log_dir = f'{run_dir}/logs_{tag_id}/tb_{d_name}_{m_name}'\n",
    "                tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                # ---- Callbacks bestmodel\n",
    "                save_dir = f'{run_dir}/models_{tag_id}/model_{d_name}_{m_name}.keras'\n",
    "                bestmodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, monitor='val_accuracy', save_best_only=True)\n",
    "                # ---- Callbacks early stopping\n",
    "                early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                                                          patience=5, verbose=1, restore_best_weights=True)\n",
    "                # ---- Train\n",
    "                start_time = time.time()\n",
    "                if datagen==None:\n",
    "                    # ---- No data augmentation (datagen=None) --------------------------------------\n",
    "                    history = model.fit(x_train, y_train,\n",
    "                                        batch_size      = batch_size,\n",
    "                                        epochs          = epochs,\n",
    "                                        verbose         = fit_verbosity,\n",
    "                                        validation_data = (x_val, y_val),\n",
    "                                        callbacks       = [tensorboard_callback, bestmodel_callback, early_stopping_callback])\n",
    "                else:\n",
    "                    # ---- Data augmentation (datagen given) ----------------------------------------\n",
    "                    datagen.fit(x_train)\n",
    "                    history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                        # steps_per_epoch = int(len(x_train)/batch_size),\n",
    "                                        epochs          = epochs,\n",
    "                                        verbose         = fit_verbosity,\n",
    "                                        validation_data = (x_val, y_val),\n",
    "                                        callbacks       = [tensorboard_callback, bestmodel_callback, early_stopping_callback])\n",
    "                    \n",
    "                # ---- Result\n",
    "                end_time = time.time()\n",
    "                duration = end_time-start_time\n",
    "                accuracy = max(history.history[\"val_accuracy\"])*100\n",
    "                #\n",
    "                output[m_name+'_Accuracy'].append(accuracy)\n",
    "                output[m_name+'_Duration'].append(duration)\n",
    "                print(f\"Accuracy={accuracy: 7.2f}    Duration={duration: 7.2f}\")\n",
    "                \n",
    "                # ---- Update checkpoint\n",
    "                if d_name not in checkpoint[\"Model\"]:\n",
    "                    checkpoint[\"Model\"][d_name] = {}\n",
    "                checkpoint[\"Model\"][d_name][m_name] = {\"Accuracy\": accuracy, \"Duration\": duration}\n",
    "                \n",
    "                # Save checkpoint\n",
    "                with open(checkpoint_file, 'w') as f:\n",
    "                    json.dump(checkpoint, f, indent=4)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Error occurred for model {m_name}: {e}')\n",
    "                output[m_name+'_Accuracy'].append('0')\n",
    "                output[m_name+'_Duration'].append('999')\n",
    "                continue\n",
    "        \n",
    "        # Mark dataset as completed\n",
    "        checkpoint[\"Dataset\"].append(d_name)\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(checkpoint, f, indent=4)\n",
    "                \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 - Run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Run --------------------------------------------------\n",
      "\n",
      "Dataset :  set-BW\n",
      "    Run model v1  : "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonydavid/Workspace/Openclassrooms/projet_6/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Accuracy=  31.98    Duration= 826.73\n",
      "\n",
      "Dataset :  set-L\n",
      "    Run model v1  : Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Accuracy=  34.88    Duration= 1377.04\n",
      "\n",
      "Dataset :  set-L-HE\n",
      "    Run model v1  : Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Accuracy=  29.65    Duration= 604.87\n",
      "\n",
      "Dataset :  set-L-LHE\n",
      "    Run model v1  : Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Accuracy=  20.35    Duration= 523.01\n",
      "\n",
      "Dataset :  set-L-CLAHE\n",
      "    Run model v1  : Restoring model weights from the end of the best epoch: 16.\n",
      "Accuracy=  36.63    Duration= 1757.17\n",
      "\n",
      "Dataset :  set-RGB\n",
      "    Run model v1  : Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Accuracy=  31.98    Duration= 902.53\n",
      "\n",
      "Dataset :  set-RGB-HE\n",
      "    Run model v1  : Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Accuracy=  33.14    Duration= 1673.67\n",
      "\n",
      "Report saved as  ./run_full/report_055225.json\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('\\n---- Run','-'*50)\n",
    "\n",
    "\n",
    "# ---- Data augmentation or not\n",
    "#\n",
    "if with_datagen :\n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n",
    "                                                           featurewise_std_normalization=False,\n",
    "                                                           width_shift_range=0.1,\n",
    "                                                           height_shift_range=0.1,\n",
    "                                                           zoom_range=0.2,\n",
    "                                                           shear_range=0.1,\n",
    "                                                           rotation_range=10.)\n",
    "else:\n",
    "    datagen=None\n",
    "# ---- Run\n",
    "#\n",
    "output = multi_run(enhanced_dir,\n",
    "                   datasets, \n",
    "                   models,\n",
    "                   datagen       = datagen,\n",
    "                   batch_size    = batch_size,\n",
    "                   epochs        = epochs,\n",
    "                   fit_verbosity = fit_verbosity,\n",
    "                   tag_id        = tag_id)\n",
    "\n",
    "# ---- Save report\n",
    "#\n",
    "report={}\n",
    "report['output']=output\n",
    "report['description'] = f' batch_size={batch_size} epochs={epochs} data_aug={with_datagen}'\n",
    "\n",
    "report_name=f'{run_dir}/report_{tag_id}.json'\n",
    "\n",
    "with open(report_name, 'w') as file:\n",
    "    json.dump(report, file, indent=4)\n",
    "\n",
    "print('\\nReport saved as ',report_name)\n",
    "\n",
    "\n",
    "print('-'*59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Show report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to find the report : \n",
    "\n",
    "report_dir = './run_full'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - Few nice functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = (s == s.max())\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_max_red(s):\n",
    "    is_max = (s == s.max())\n",
    "    return ['background-color: red' if v else '' for v in is_max]\n",
    "\n",
    "def show_report(file):\n",
    "    # ---- Read json file\n",
    "    with open(file) as infile:\n",
    "        dict_report = json.load( infile )\n",
    "    output      = dict_report['output']\n",
    "    description = dict_report['description']\n",
    "    # ---- about\n",
    "    display(Markdown(f\"<br>**Report : {Path(file).stem}**\"))\n",
    "    print(    \"Desc.  : \",description,'\\n')\n",
    "    # ---- Create a pandas\n",
    "    report       = pd.DataFrame (output)\n",
    "    col_accuracy = [ c for c in output.keys() if c.endswith('Accuracy')]\n",
    "    col_duration = [ c for c in output.keys() if c.endswith('Duration')]\n",
    "    # ---- Build formats\n",
    "    lambda_acc = lambda x : '{:.2f} %'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n",
    "    lambda_dur = lambda x : '{:.1f} s'.format(x) if (isinstance(x, float)) else '{:}'.format(x)\n",
    "    formats = {'Size':'{:.2f} Mo'}\n",
    "    for c in col_accuracy:   \n",
    "        formats[c]=lambda_acc\n",
    "    for c in col_duration:\n",
    "        formats[c]=lambda_dur\n",
    "    # t=report.style.highlight_max(subset=col_accuracy).format(formats)\n",
    "    t = report.style.apply(highlight_max_red, subset=col_accuracy).format(formats)\n",
    "    display(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 - Reports display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<br>**Report : report_060218**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desc.  :   batch_size=16 epochs=20 data_aug=False \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_53735_row0_col3, #T_53735_row1_col1, #T_53735_row2_col5, #T_53735_row5_col1 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_53735\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_53735_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
       "      <th id=\"T_53735_level0_col1\" class=\"col_heading level0 col1\" >v2_Accuracy</th>\n",
       "      <th id=\"T_53735_level0_col2\" class=\"col_heading level0 col2\" >v2_Duration</th>\n",
       "      <th id=\"T_53735_level0_col3\" class=\"col_heading level0 col3\" >v3_Accuracy</th>\n",
       "      <th id=\"T_53735_level0_col4\" class=\"col_heading level0 col4\" >v3_Duration</th>\n",
       "      <th id=\"T_53735_level0_col5\" class=\"col_heading level0 col5\" >v4_Accuracy</th>\n",
       "      <th id=\"T_53735_level0_col6\" class=\"col_heading level0 col6\" >v4_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_53735_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_53735_row0_col0\" class=\"data row0 col0\" >set-BW</td>\n",
       "      <td id=\"T_53735_row0_col1\" class=\"data row0 col1\" >33.72 %</td>\n",
       "      <td id=\"T_53735_row0_col2\" class=\"data row0 col2\" >529.9 s</td>\n",
       "      <td id=\"T_53735_row0_col3\" class=\"data row0 col3\" >27.33 %</td>\n",
       "      <td id=\"T_53735_row0_col4\" class=\"data row0 col4\" >47.4 s</td>\n",
       "      <td id=\"T_53735_row0_col5\" class=\"data row0 col5\" >29.07 %</td>\n",
       "      <td id=\"T_53735_row0_col6\" class=\"data row0 col6\" >174.8 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53735_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_53735_row1_col0\" class=\"data row1 col0\" >set-L</td>\n",
       "      <td id=\"T_53735_row1_col1\" class=\"data row1 col1\" >35.47 %</td>\n",
       "      <td id=\"T_53735_row1_col2\" class=\"data row1 col2\" >256.3 s</td>\n",
       "      <td id=\"T_53735_row1_col3\" class=\"data row1 col3\" >26.74 %</td>\n",
       "      <td id=\"T_53735_row1_col4\" class=\"data row1 col4\" >124.4 s</td>\n",
       "      <td id=\"T_53735_row1_col5\" class=\"data row1 col5\" >27.33 %</td>\n",
       "      <td id=\"T_53735_row1_col6\" class=\"data row1 col6\" >127.7 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53735_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_53735_row2_col0\" class=\"data row2 col0\" >set-L-HE</td>\n",
       "      <td id=\"T_53735_row2_col1\" class=\"data row2 col1\" >31.40 %</td>\n",
       "      <td id=\"T_53735_row2_col2\" class=\"data row2 col2\" >307.6 s</td>\n",
       "      <td id=\"T_53735_row2_col3\" class=\"data row2 col3\" >25.58 %</td>\n",
       "      <td id=\"T_53735_row2_col4\" class=\"data row2 col4\" >80.0 s</td>\n",
       "      <td id=\"T_53735_row2_col5\" class=\"data row2 col5\" >33.72 %</td>\n",
       "      <td id=\"T_53735_row2_col6\" class=\"data row2 col6\" >274.8 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53735_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_53735_row3_col0\" class=\"data row3 col0\" >set-L-LHE</td>\n",
       "      <td id=\"T_53735_row3_col1\" class=\"data row3 col1\" >25.58 %</td>\n",
       "      <td id=\"T_53735_row3_col2\" class=\"data row3 col2\" >180.5 s</td>\n",
       "      <td id=\"T_53735_row3_col3\" class=\"data row3 col3\" >25.00 %</td>\n",
       "      <td id=\"T_53735_row3_col4\" class=\"data row3 col4\" >40.8 s</td>\n",
       "      <td id=\"T_53735_row3_col5\" class=\"data row3 col5\" >19.19 %</td>\n",
       "      <td id=\"T_53735_row3_col6\" class=\"data row3 col6\" >117.9 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53735_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_53735_row4_col0\" class=\"data row4 col0\" >set-L-CLAHE</td>\n",
       "      <td id=\"T_53735_row4_col1\" class=\"data row4 col1\" >31.98 %</td>\n",
       "      <td id=\"T_53735_row4_col2\" class=\"data row4 col2\" >430.8 s</td>\n",
       "      <td id=\"T_53735_row4_col3\" class=\"data row4 col3\" >19.77 %</td>\n",
       "      <td id=\"T_53735_row4_col4\" class=\"data row4 col4\" >54.9 s</td>\n",
       "      <td id=\"T_53735_row4_col5\" class=\"data row4 col5\" >33.14 %</td>\n",
       "      <td id=\"T_53735_row4_col6\" class=\"data row4 col6\" >213.8 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53735_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_53735_row5_col0\" class=\"data row5 col0\" >set-RGB</td>\n",
       "      <td id=\"T_53735_row5_col1\" class=\"data row5 col1\" >35.47 %</td>\n",
       "      <td id=\"T_53735_row5_col2\" class=\"data row5 col2\" >274.3 s</td>\n",
       "      <td id=\"T_53735_row5_col3\" class=\"data row5 col3\" >19.19 %</td>\n",
       "      <td id=\"T_53735_row5_col4\" class=\"data row5 col4\" >51.9 s</td>\n",
       "      <td id=\"T_53735_row5_col5\" class=\"data row5 col5\" >32.56 %</td>\n",
       "      <td id=\"T_53735_row5_col6\" class=\"data row5 col6\" >248.5 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_53735_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_53735_row6_col0\" class=\"data row6 col0\" >set-RGB-HE</td>\n",
       "      <td id=\"T_53735_row6_col1\" class=\"data row6 col1\" >31.98 %</td>\n",
       "      <td id=\"T_53735_row6_col2\" class=\"data row6 col2\" >358.0 s</td>\n",
       "      <td id=\"T_53735_row6_col3\" class=\"data row6 col3\" >19.19 %</td>\n",
       "      <td id=\"T_53735_row6_col4\" class=\"data row6 col4\" >59.5 s</td>\n",
       "      <td id=\"T_53735_row6_col5\" class=\"data row6 col5\" >31.98 %</td>\n",
       "      <td id=\"T_53735_row6_col6\" class=\"data row6 col6\" >354.5 s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x312f006d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<br>**Report : report_055225**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desc.  :   batch_size=16 epochs=20 data_aug=False \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7b9e0_row4_col1 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7b9e0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7b9e0_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
       "      <th id=\"T_7b9e0_level0_col1\" class=\"col_heading level0 col1\" >v1_Accuracy</th>\n",
       "      <th id=\"T_7b9e0_level0_col2\" class=\"col_heading level0 col2\" >v1_Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7b9e0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7b9e0_row0_col0\" class=\"data row0 col0\" >set-BW</td>\n",
       "      <td id=\"T_7b9e0_row0_col1\" class=\"data row0 col1\" >31.98 %</td>\n",
       "      <td id=\"T_7b9e0_row0_col2\" class=\"data row0 col2\" >826.7 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b9e0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7b9e0_row1_col0\" class=\"data row1 col0\" >set-L</td>\n",
       "      <td id=\"T_7b9e0_row1_col1\" class=\"data row1 col1\" >34.88 %</td>\n",
       "      <td id=\"T_7b9e0_row1_col2\" class=\"data row1 col2\" >1377.0 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b9e0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7b9e0_row2_col0\" class=\"data row2 col0\" >set-L-HE</td>\n",
       "      <td id=\"T_7b9e0_row2_col1\" class=\"data row2 col1\" >29.65 %</td>\n",
       "      <td id=\"T_7b9e0_row2_col2\" class=\"data row2 col2\" >604.9 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b9e0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7b9e0_row3_col0\" class=\"data row3 col0\" >set-L-LHE</td>\n",
       "      <td id=\"T_7b9e0_row3_col1\" class=\"data row3 col1\" >20.35 %</td>\n",
       "      <td id=\"T_7b9e0_row3_col2\" class=\"data row3 col2\" >523.0 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b9e0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7b9e0_row4_col0\" class=\"data row4 col0\" >set-L-CLAHE</td>\n",
       "      <td id=\"T_7b9e0_row4_col1\" class=\"data row4 col1\" >36.63 %</td>\n",
       "      <td id=\"T_7b9e0_row4_col2\" class=\"data row4 col2\" >1757.2 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b9e0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7b9e0_row5_col0\" class=\"data row5 col0\" >set-RGB</td>\n",
       "      <td id=\"T_7b9e0_row5_col1\" class=\"data row5 col1\" >31.98 %</td>\n",
       "      <td id=\"T_7b9e0_row5_col2\" class=\"data row5 col2\" >902.5 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b9e0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7b9e0_row6_col0\" class=\"data row6 col0\" >set-RGB-HE</td>\n",
       "      <td id=\"T_7b9e0_row6_col1\" class=\"data row6 col1\" >33.14 %</td>\n",
       "      <td id=\"T_7b9e0_row6_col2\" class=\"data row6 col2\" >1673.7 s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x304b428d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for file in glob.glob(f'{report_dir}/report_*.json'):\n",
    "    show_report(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
